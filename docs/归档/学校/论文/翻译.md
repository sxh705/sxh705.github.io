IEEE/ACM transactions on networking, vol. 27, no.3, JUNE 2019
网络高速缓存设计的实用优化方法
Mostafa Dehghan , Laurent Massoulié, Don Towsley , Life Fellow, IEEE, Fellow, ACM,
Daniel Sadoc Menasché, and Y. C. Tay
摘要-- 在任何缓存系统中，接纳和剔除策略都决定了在发生缺失时从缓存中添加和删除哪些内容。通常情况下，制定这些策略的目的是为了减少滞留并提高命中概率。然而，不同内容的高命中概率的效用可能会有所不同。例如，当必须满足服务水平协议或某些内容比其他内容更难获取时，就会出现这种情况。在本文中，我们提出了效用驱动型缓存，即为每个内容关联一个效用，该效用是相应内容命中概率的函数。我们提出了优化问题，其目标是最大化所有内容的效用总和。这些问题根据缓存容量约束的严格程度而有所不同。我们的框架使我们能够反向设计经典的替换策略，如 LRU 和 FIFO，计算它们最大化的效用函数。我们还开发了在线算法，服务提供商可利用这些算法根据任意效用函数实施各种缓存策略。
Index Terms- Utility maximization, caching policy.
I. 导言据预测，过去几年数据流量的增长势头将更加强劲，全球互联网流量将达到据估计，2019 年的数据流量将达到 2005 年的 64 倍[1]。数据流量增长的主要原因是通过蜂窝网络传输视频点播内容。然而，增加频谱或部署更多基站等传统方法不足以应对预计的流量增长[2], [3]。在当前和未来的互联网架构提案中，缓存被认为是提高网络应用性能的最有效手段之一。通过使内容更接近用户，缓存大大减少了网络带宽的使用、服务器负载和感知到的服务延迟[4]。
手稿于 2017 年 2 月 6 日收到；2018 年 6 月 3 日修订，并
2018 年 12 月 27 日；2019 年 3 月 19 日接受；IEEE/ACM TRANSACTIONS
ON NETWORKING 编辑 J. Shin 批准。出版日期：2019 年 5 月 15 日；当前版本日期：2019 年 6 月 14 日。这项工作得到了美国国家科学基金会
CNS-1413998 号资助和 CNS-1617437 号资助的支持。(通讯作者：Mostafa Dehghan）。
M.Dehghan 现供职于谷歌公司，地址：Cambridge, MA 02142-1493
USA（电子邮件：dehghan.mostafa@gmail.com）。
L.Massoulié 现供 职于 微软 研究院 -INRIA 联合 中心 ， 地址 ： 91120
Palaiseau, France（电子邮箱：laurent.massoulie@inria.fr）。
D.Towsley 现为马萨诸塞大学阿默斯特分校信息与计算机科学学院
（College of Information and Computer Sciences, University of MassachusettsAmherst, Amherst, MA 01003 USA）（电子邮箱：towsley@cs.umass.edu）。
D.S. Menasché现供职于里约热内卢联邦大学计算机科学系，邮编：21941-901，巴西里约热内卢（电子邮箱：sadoc@dcc.ufrj.br）。
Y.Y. C. Tay 现为新加坡国立大学数学系，新加坡 119077（电子邮箱：dcstayyc@nus.edu.sg）。
由于云计算的趋势、新内容发布者和消费者的产生，互联网正日益成为一个异构环境，不同的内容类型有不同的服务质量要求，具体取决于内容发布者/ 消费者。
预计在不久的将来，随着智能楼宇和网络家电的发展，新的数据源将不断涌现，内容和消费者也将更加多样化
[5] 。服务期望的日益多样化要求内容交付基础设施在不同应用和内容类别之间提供差异化服务。此外，服务还具有经济价值。因此，服务差异化不仅能带来重要的技术收益，还能带来显著的经济效益[6]。
尽管对通信网络中公平、高效的带宽共享算法的设计和实施进行了大量研究，但很少有人关注在网络和网络缓存中提供多级服务。现有的少量研究主要集中在设计缓存空间分区控制器[5]、[7]、针对特定内容类别的偏向替换策略[8]或使用多级缓存[6]。这些技术要么需要额外的控制器来保证公平性，要么不能有效利用缓存存储。
此外，传统的缓存管理策略（如 LRU ）以强耦合的方式处理不同的内容，这使得（缓存）服务提供商难以实施差异化服务，内容发布商也难以对通过内容分发网络分发的内容进行估值。
在本文中，我们提出了一种效用驱动型缓存框架，其中每个内容都有一个相关效用，内容在缓存中的存储和管理是为了使所有内容的总效用最大化。可以选择效用来权衡用户满意度和在缓存 中存储内容的成本。我们借鉴了实时缓存（TTL ）的分析结果[9] ，为 单个（或多类）内容设计了与效用相关联的缓存。效用函数还具有隐含的公平性概念，它规定了每个内容在缓存 中的停留时间。请注意，这种公平性概念可应用于网络（边缘）缓存，在网络
（边缘）缓存中，移动设备或接入点充当缓存，以较低的延迟向用户高效地传送内容。我们的框架使我们能够开发缓存管理的在线算法，并证明这些算法能达到最佳性能。
我们的框架对分布式定价和控制机制有影响，因此非常适合设计缓存市场经济模型。
我们在本文中的主要贡献可概括如下：
• 我们制定了一个基于效用的优化框架，在服务提供商的缓冲容量限制下最大化内容发布者的总效用。我们表明，现有的缓存策略（如 LRU、LFU 和 FIFO）可在此框架内建模为效用驱动缓存。
• 通过将 LRU 和 FIFO 缓存政策逆向工程为效用最大化问题，我们发现与缓存容量约束相对应的拉格朗日乘数与缓存特征时间的概念有关，该概念于 1977 年由 Fagin [10] 作为窗口大小首次提出，并于 2001 年由 Che 等人 [11] 重新发现。
• 我们开发了管理高速 缓存 的在线算法，并利用Lyapunov 函数证明了这些算法对最优解的收敛性。
• 我们表明，我们的框架可用于基于收入的模型，在这种模型中，内容发布商会对（缓存）服务提供商设定的价格做出反应，而不会透露其效用函数。
• 我们使用不同的效用函数和不同的公平概念进行了模拟，以展示我们在线算法的效率。
本文的其余部分安排如下 。我们在下一节回顾了相关工作。第三节解释本文考虑的网络模型，第四节介绍我们设计效用最大化缓存的方法。在第五节，我们阐述了效用函数对公平性的影响；在第六节，我们推导了 LRU 和 FIFO 缓存最大化的效用 函数 。第七节，我们开发了实现效用最大化缓存的在线算法。我们在第八节介绍了仿真结果，并在第九节讨论了缓存效用最大化框架的前景和意义。最后，我们在第 X 节总结本文。
II. 相关工作
A. 网络实用性和服务差异化效用函数已被广泛应用于计算机网络的建模和 控制，从队列的稳定性分析到网络资源分配中的公平性和服务差异化研究；参见 [12]、[13] 及其中的参考文献。Kelly [14]
首次将速率分配问题表述为实现用户总效用最大化的问题，并描述了如何通过让单个用户控制其传输速率来实现全网最优速率分配。Kelly 等人[15] 的研究首次提出了一般拓扑网络拥塞控制算法行为的数学模型和分析。此后，人们开始广泛研究如何推广和应用 Kelly 的网络效用最大化框架，对各种网络协议和架构进行建模和分析。这一框架已被用于研究网络路由[16]、吞吐量最大化[17]、动态功率分配 [18] 和能量收集网络中的调度 [19] 等等。
网络缓存管理中的服务差异化问题也得到了广泛的研究（例如，见 [6] 及其中的参考文献）。不过，有关这一主题的大多数研究都使用缓存分区作为提供服务差异化的手段[20]-[22]。Ma 和 Towsley [23]最近提议使用效用函数来设计合同，使服务提供商能够将缓存货币化。
我们在之前的工作 [24] 的基础上，引入了一种效用驱动型方法，根据与每个内容相关的效用来管理缓存内容。
在 [25] 和 [26] 中 ，Neglia 等人扩展了本文提出的框架，以处理不同大小的内容，并使用跟踪驱动的模拟来评估他们的算法。
B. 实时缓存在 TTL 缓存中，内容会在定时器到期时被驱逐，这种缓存从互联网诞生之初就开始使用，域名系统（DNS）
就是其中的一个重要应用[27]。最近，TTL 缓存重新受到人们的欢迎，这主要是因为在缓存分析中采用了一种通用方法，这种方法也可用于 对基于替换的缓存策略（
如 LRU）进行建模。TTL 缓存和基于替换（容量驱动）
的策略之间的联系，Fagin[10]和 Che 等人[11]分别通过赢道大小和缓存特征时间的概念为 LRU 策略建立了联系
。特征时间在理论上是合理的，并扩展到其他缓存策略，如 FIFO 和 RANDOM [28]。这一联系被进一步证实适用于比泊松过程更一般的到达模型 [29]。在过去几年中，已经提出了几种精确和近似的分析方法，用于单独模拟单个高速缓存，以及使用 TTL 框架模拟高速缓存网络
[30], [31]。最近，Ferragut 等人[32]、[33] 提出了一个优化问题，以确定如何选择最大化命中概率的定时器，并确定了重尾到达情况下基于 TTL 的最优策略结构。
Neglia 等人[26] 研究了具有线性成本函数的内容检索缓存问题，并提出了解决线性效用最大化问题的动态策略
。在[25]中，Neglia 等人使用效用最大化方法提出了一种新的缓存替换策略，该策略利用了内存和磁盘之间存在访问时间差的分层缓存架构。Wang 等人[34]采用基于效用的方法，将缓存问题建模为纳什讨价还价博弈。Chu
等人[35]在向内容提供商分配缓存资源时提出了一种效用驱动的缓存分区方法。
在本文中，我们使用 TTL 定时器作为单个（或类别）
文件的调节旋钮，以控制相应内容所观察到的效用，并在不同（类别）内容之间实现缓存空间的公平使用。我们基于下一节描述的两种 TTL 缓存来 开发我们的框架。
表 I
术语表
IV. 高速缓存效用最大化在本节中，我们将缓存管理表述为一个效用最大化问题
。我们引入了两种表述方式，一种是缓冲区大小固定不变，另一种是我们可以增加缓冲区大小，但会产生额外的存储成本。
A.固定缓存大小我们感兴趣的是设计一种缓存管理策略，它能更精确地优化所有文件的效用总和、
III.
型号考虑一个大小为 B 的高速缓存，为一组 N 个文件提供服务。我们假设对文件 i 的请求是一个泊松过程，其速率为 λi 。此外，文件 i 的大小为 si 。让 hi 表示内容
i 的命中概率。与每个内容（i = 1，...，N）相关的是一个效用函数 Ui : [0, 1] → R
最大化嗨
以致Ui (h)ii=10 ≤ hi ≤ 1，i = 1，2，...，N。
(3)
1iUi (hi) 假设为递增、连续可微分和严格凹形。我们进一步假设 Ui′(0) = 。这确保了 hi > 0，i。请注意，具有这些性质的函数是可逆的。我们将处理不满足这些限制条件的效用函数是特例。
A. TTL 缓存在 TTL 缓存中，每个内容都与定时器 ti 相关联。每当内容 i 出现缓存缺失时，内容 i 就会被存储到缓存中，其定时器也会被设置为 ti 。定时器会以恒定的速率递减，当定时器为零时，内容就会被从缓存中删除。我们可以通过控制文件在缓存中的保留时间来调整文件的命中概率。
有两种 TTL 高速缓存设计：
• 非复位 TTL 高速缓存：仅在缓存未命中时设置 TTL即 TTL 不会在高速缓存命中时重置。
• 重置 TTL 缓存：每次请求内容时都会设置 TTL。
以前的 TTL 缓存分析工作（见 [27]、[37]）表明，对于这两类非重置和重置 TTL 缓存，文件 i 的命中概率可表示为请注意，可行解集是凸的，由于目标函数是严格凹连续的，因此存在一个唯一的最大值，即最优解。还要注意的是，缓冲区约束是基于不超过缓冲区大小的预期文件数，而不是文件总数。在本节末尾，我们将展示缓冲区空间的管理方式，即当文件数和缓冲区大小增大时，违反缓冲区大小约束的概率将消失。
在上述表述中，效用被定义为文件命中概率的函数，即 Ui (hi)。在第九节中，我们认为效用也可以定义为字节命中概率的函数，即 Ui (s h)。ii
上述表述并不强制要求采用任何特殊技术来管理缓存内容，以达到所需的 hi s，而且可以采用任何能够轻松调整命中概率的策略。我们使用 TTL 缓存作为构建模块，因为它提供了通过设置定时器来控制不同文件命中概率的方法，从而使效用总和最大化。
使用基于定时器的缓存技术来控制命中概率，0 < ti
< 确保 0 < hi < 1，因此，不考虑 hi = 0 或 hi = 1
的可能性。
(4)表 I 列出了本文所用主要符号的术语表，以帮助记号。
让 Ui′（-）表示效用函数的导数
Ui (-)，并定义 U′−1 (-) 为其反函数。由 (4) 可知
1 在本文中，我们将效用定义为高速缓存命中概率的函数。Panigrahy
等人[36]研究了将效用定义为命中率的函数。
(5)
C. 违反缓冲区限制在结束本节讨论之前，我们先讨论一下在这两种公式中都会出现的一个问题，即如何处理以下事实应用缓存存储约束
(6)
缓冲区中可能无法存储更多的未到期定时器内容。这种情况出现在 (3) 的表述中，因为约束条件是缓冲区的平均占用率，而 (4) 的表述则是缓冲区的平均占用率。
在 (9) 中，因为没有约束条件。让我们关注和 α 可以通过求解上面给出的定点方程计算出来。
如前所述，我们可以使用基于 TTL 的策略来实现效用最大化缓存。利用 (1) 和 (2) 中给出的非重置和重置 TTL
缓存的命中概率表达式，我们可以计算出定时器参数 ti ，只要根据 (6) 确定了 α。对于非重置 TTL 缓存，我们可以得到
(3) 中的公式。我们的方法是提供一个大小为 B(1 + ϵ)
、ϵ > 0 的缓冲区，其中一部分 B 用于解决优化问题，另一部分 ϵB 用于处理缓冲区违规问题。我们将看到，随着内容数量 N 的增加，我们可以让 B 以亚线性方式增长，并让ϵ 缩小为零，同时确保内容在其定时器到期前不会被高概率地从缓存中驱逐出去。让 Xi 表示是否内容 i 是否在缓存中；P (X = 1) = h 。
对于重置 TTL 缓存，我们可以得到的函数，并假设 B(N) = ω(1)。
定理 1：假设 si ≤ smax , ∀i.对于任意 ϵ > 0
B. 弹性缓存大小
(3) 中的表述假定缓存容量是固定的。在某些情况下，（高速缓存）服务提供商可以在以下位置增加可用的高速缓存存储空间证明来自切尔诺夫约束的应用。定理 1 指出，我们可以将缓冲区的大小设为 B(1 + ϵ)，同时在优化过程中使用部分 B 作为约束条件。剩下的ϵB 部分用于防止违反缓冲区约束。就我们的目的而言，只要文件提供者需要为额外的资源支付一定的费用2 。在这种情况下，缓存容量限制可替换为
2 B(N)最大值ω(1).这样我们就可以选择B（N) = o(N) 而惩罚函数 C() 表示额外缓存存储的成本。这里，C()被假定为凸函数和递增函数。现在，我们可以将效用和成本驱动的缓存公式写成同时选择 ϵ = o(1)。举例说明齐普夫定律，λi = λ/iz ，λ > 0，0 < z < 1，i = 1，...，N，假设 max ti = t，对于某个 t < 。在这种情况下，我们可以以 B(N) = O(N1−z) 的方式增长缓冲区，而 ϵ 可以以 ϵ = 1/N(1−z)/3 的方式缩小。类似表达式N
可以得出 z ≥ 1 时的最大化Ui (hi) - C(s hii -
B)
对于具有弹性的配方，也可以做出类似的选择。
这样
0 ≤ hi ≤ 1，i = 1，2，...，N。
(9)
注意上述优化问题的最优条件是

缓存大小=(7)
让 E
ti

ϵ)i
V. 效用函数与公平在优化公式 (3) 中使用不同的效用函数，会产生不同的文件计时器值。从这个意义上说，每个效用函数都定义了为不同文件分配存储资源的公平性概念。在本节中、
因此，对于命中概率，我们可以得到在这里，我们考虑的是β - 公平（也称为等弹性）效用函数如下

,, i

1-β
β ≥
将上式两边乘以 si 并对所有 i 求和，我们就可以利用定点方程计算出缓存存储的最佳值 B∗ = i s h ii

其中系数 wi 0 表示文件 i 的权重。这个效用函数族统一了资源分配公平性的不同概念[12]。请注意，求解优化
B∗ =

Ui′
-1

) .
(10)
根据上述定义的效用函数，可以得出（3）中的问题的命中概率为
w1/β
2 一种直接的思路是将高速缓冲存储器磁盘

根据需求开启或关闭。

1/β我们研究了一些具有重要公平特性的效用函数。

si Σ

表 II
Β - 公平效用函数族在本节的其余部分，我们将研究一些导致有趣特例的 β
选择。
文件 i 的命中概率等于

wi

当 β = 0 时，我们得到 Ui (hi) = w hii ，效用总和最大化对应于最大

上述效用函数的定义不符合第三节中提到的对效用 函数 的要求，因为它不是严格的凹函数。然而，不难看出这一效用函数实现了比例公平政策 [15]。当 wi = λi
时，文件 i 的命中概率与 λ /sii 成比例。

当 β = 2 时，我们得到 Ui (hi) = w /hii ，总效用最大化对应于

hi = 1,i = 1,...,m - 1m-1
在这种情况下，我们得到 U′−1 (αsi) = √wi /√αsi ，因此s Ui′ -1(αsi) =√w sii /√α = B、hm = (B -)
i=1

因此α = Σ

其中文件排序为 w /s

文件 i 的命中概率等于
= U′ -1(αs) = √w i = √wi B．√wi B.
已知上面定义的效用函数会产生最小--
需要注意的是，用 wi = λi 实现此效用函数后得到的策略能使整体吞吐量最大化。如果所有文件的大小相同，即 si = s, i，则该策略对应于最不频繁使用 (LFU)
缓存策略。

让 β = 1，我们得到 Ui (hi) = wi log hi ，因此效用总和最大化对应于

TCP 拥塞控制协议实现了这样一个实用功能。

由于 Ui ()的定义是凹函数，因此当 β
优化问题 (3) 和 (9) 的解收敛于最大最小公平分配（证明见 [38]）。因此，我们得到
hi = min {1, s N }, ∀i.i
本文讨论的效用函数简述如下不难看出 U′−1 (αs) = wi ，因此使用 (6)
见表 II。i{i=1nhi = 0,i = m + 1,...,
N,

B}
hiN
最大潜在延迟公平性。文献[15]表明
我们得到i

B、
VI. 逆向工程在本节中，我们将研究广泛使用的基于替换的高速缓存策略 FIFO 和 LRU，并证明它们的命中率/容错率（hit/miss
）都很高。得出α =

通过适当选择效用函数，我们的框架可以复制这些行为。
文献[30]指出，通过适当选择定时器值，TTL 高速缓存可以近似地产生与 TTL 高速缓存相同的数据。
hi
- /
λ Ti
hi
- e T−Σ-i.-hi = U′ -1(αs /λii).
dt

αs与 FIFO 和 LRU 缓存策略具有相同的统计特性，即相同的命中/遗漏概率。在实施这些缓存时，FIFO 和 LRU
分别使用非复位和复位 TTL 缓存，ti = T，i = 1，...，N，其中 T 表示这些缓存的特征时间 [11]。对于泊松到达的 FIFO 和 LRU 缓存，命中概率可表示为 = 1 1 (1
+) 和 = 1 λiT ，以及计算出 i s hii = B。 例如，对于 LRU 策略，T 是定点方程的唯一解si
i=1

图 1. 与 LRU 和 FIFO 缓存策略相关的效用函数。
在我们的框架中，我们可以从 (5) 中看出，文件命中概率取决于与 (3) 中缓存大小约束相对应的拉格朗日乘数 α
。这表明 T 和 α 之间存在联系。进一步注意，命中概率是
T 的递增函数。另一 方面 ，由于效用函数是凹进和递增的，hi = U′−1 (αsi) 是得出
U′ (x) = -λisi 。ln (1 - x)
对上式两边进行积分，得出的递减函数α.因此，我们可以将 T 表示为 a
LRU 缓存策略的效用函数的递减函数，即 T = f (α)。
函数 f () 的不同选择会导致先进先出政策和后进先出政策的效用函数不同。然而，如果我们强加函数依赖关系
Ui (hi) = λ Ui0 (hi) ，那么等式 hi = U′−1 (αsi) 得出
Ui (hi) = λ sii li(1 - hi)、
其中 li(-) 是对数积分函数
∫ x
根据 FIFO 和 LRU 策略的命中概率表达式，我们可以得到 T = 1/α。在本节的其余部分，我们将利用这一点推导出 FIFO 和 LRU 策略的效用函数。
A. 先进先出在特征时间为 T 的先进先出缓存中，请求率为 λi 的文件 i 的命中概率为

将其代入 (5) 并让 T = 1/α 即可得出
′-1(

Computing the inverse of U ′−1(·) yields

对上式两边进行积分，就得出了先进先出缓存的效用函数
Ui (hi) = λ sii log hi - hi 。

取 hi = 1
e−λ iT 为 LRU 策略，并让
T = 1/α 得到
U′ -1(αsi) = 1 - e−λ i/α
(13)

使用第四节中解释的方法很容易验证，上面计算的效用函数确实得出了 FIFO 和 LRU 缓存命中概率的正确表达式。我们认为，如果限制这些效用函数在3 λi 中为乘法，那么它们就是唯一的。
图 1 描述了在 LRU 和 FIFO 缓存中，si = 1 和 λi = 1
的文件命中概率的效用函数。
VII.
在线算法在第四节中，我们将效用驱动型缓存表述为一个凸优化问题，缓存大小可以是固定的，也可以是弹性的。然而，离线求解优化问题并实施最优策略并不可行。此外，系统参数会随着时间的推移而变化。因此，我们需要能通过收集有限信息来实施最优策略并适应系统变化的算法。在本节中，我们将开发这样的算法。
A. 双重解决方案问题（3）中提出的效用驱动缓存是一个凸优化问题，因此对偶差距为零，即求解对偶问题可以找到最优解。
问题 (3) 的拉格朗日对偶问题是通过以下方法将约束条件纳入最大化求解过程中得到的
3 我们注意到，在这种情况下定义的效用函数受仿射变换的影响，即对于任何常数 a > 0 和 b，aU + b 产生的命中概率与 U 相同。

∂α
= -
其中 γ > 0 控制每次迭代的步长。注意iiii
λii
拉格朗日乘法器N
让 α∗ 表示 α 的最优值。
(α) - D(α) 是 Lyapunov 函数，且
(Σ
"Σ附录 B，D使得 α ≥ 0, ν , η ≥ 0。

)ii
B. 原始解决方案现在，我们考虑一种基于 (9) 所述优化问题的算法，即所谓的基元算法。
让 W (h) 表示 (9) 中的目标函数，其定义为使用基于定时器的缓存技术控制命中率概率为 0 < ti < ∞，确保 0 < hi < 1，并且N
因此，我们有 νi = 0 和 ηi = 0W (h) = Σ
Ui (hi) - C(s hii - B)。
来解决效用最大化问题 (3)。注意，在 νi = 0 和 ηi = 0 的情况下，我们可以把效用最大化问题的拉格朗日对偶问题写成获取 W (h) 最大值的一个自然方法是使用梯度上升算法
。梯度上升算法的基本思想是将变量 hi 在

梯度的方向min D(α).
α≥0
要最小化 D(α)，一种自然的分散方法是使用梯度下降算法将决策变量逐步移向最佳点。梯度的计算方法如下
∂D(α)由于命中概率由 TTL 定时器控制，我们通过更新 ti 使
hi 向最佳点移动。让 h˙i 表示命中概率 hi 随时间的导数
。同样，将 t˙i 定义为定时器参数 ti 相对于时间的导数
。我们有

由于我们进行的是梯度下降，α 应根据梯度的负值进行更新，即

非复位和复位 TTL 缓存。因此，将 ti 移至梯度的方向，也会使 hi 向该方向移动。
通过梯度上升法，定时器参数应根据以下公式更新t ← max n0,t + k hU′ (h) - s C′ (B - B)
i,KKT 条件要求 α ≥ 0。
其中，ki > 0 为步长参数，ΣN

根据第 IV 节的讨论，要满足最优条件，我们必须有改为 B
双重解决方案。
i=1
c基于与或等同于

、i

让 h∗ 表示 (9) 的最优解。我们将在附录 C 中证明 W (h∗
) W (h) 是一个 Lyapunov 函数，上述算法会收敛到最优解然后根据定时器参数 ti 控制命中概率，对于非复位和复位
TTL 缓存，定时器参数可根据 (7) 和 (8) 进行设置。
将命中概率视为缓存中文件的指标，表达式 i s hii 可解释为当前缓存中的项目数，在此表示为如 Bc 。因此，我们可以将复位 TTL 算法的控制算法概括为
t = - 1 log 1 - U′ -1(αs) 、

上述算法就会收敛到最优解。
i=1
i=1
从 (1) 和 (2) 很容易确认 ∂h /∂ti
i > 对于减少

= 最大值嗨
i=1
i=1
在此，我们考虑一种基于对偶解的算法嗨
i=1
i=1
∂W (h) = U′ (h) - s C (′

对偶问题可写成

C. 原始-双重解决方案在此，我们考虑第三种算法，它综合了前两种算法的要素。考虑控制算法
ti ← max {0, ti + ki [Ui′(hi) - αsi
]}，α ← max {0,α + γ(Bc - B)}我们在附录 D 中利用 Lyapunov 技术证明，上述算法收敛于最优解。
现在，不用再根据
α ← max {0,α + γ(Bc - B)}.
(15)
通过使用 (7) 中 t 的正确表达式i ，我们得到了非复位 TTL
高速缓存的算法。
有基于缓存命中或未命中的更新规则。考虑以下微分方程
t˙i = δm (ti , α)(1 - hi)λi - δh (ti , α)hi λi 、
(16)
根据效用函数来明确上述规则，我们可以
√-i在 B 处的占用率，得出 hi = B/
j sj¯¯其中，δm (ti , α) 和 δh (ti , α) 分别表示文件 i 的缓存未命中或命中时 ti 的变化。更具体地说，文件 i 的计时器在缓存未命中时增加 δm (ti , α)，在缓存命中时减少 δh (ti ,
α)。
当 t˙i = 0 时，会出现 (16) 的平衡点，求解 hi 可得
hi =

(ti , α)+ δh (ti , α) .
(17)
将上述表达式与 hi = U′−1 (αsi) 相比较，可以看出这表明图 2. ( a) 违反高速缓存大小的概率和 (b) 额外高速缓存的百分比δm (ti , α) = U′ -1(αsi) 和 δh (ti , α) = 1 - U′ -1(αsi)、
缓冲空间随着系统规模的扩大而减少，因为 B =
N 和 є = √5 N。
可以达到理想的命中概率和缓存策略。
请注意，根据效用函数的不同，将 δm (ti , α) 和 δh (ti , α)
乘以一个正实体可能会得到更简单的表达式。例如，为了实现比例公平，我们使用
δm (ti , α) = λi ， δh (ti , α) = αsi - λi 。
(18)
需要注意的是，这里我们将原始表达式中的
δm (ti , α) 和 δh (ti , α) 由 αs 。i
在最大-最小公平性的情况下，我们希望在 hi = B/ j sj
时使命中概率相等。要做到这一点，可以让 ti 演化，当 hi 低于共享值时，它就会增加。
而当高于该值时，定时器 t 就会减小。要实现这种动态功能，可以在文件 i 的每次缓存未命中时将定时器 ti 增加��个单位，并在文件 i 的每次请求导致��存命中时将定时器 t 减少 α 1，即
δm (ti , α) = 1，且 δh (ti , α) = α - 1。
从 (17) 可以看出，在平衡状态下，所有文件都具有相同的

模拟在本节中，我们将通过实验来理解第 IV 节中开发的框架的含义，并评估第 VII 节中开发的在线算法的效率。在本节中，我们假设文件为单位大小，即 si = 1, ∀i。
A. 缓存大小违规在我们的效用驱动公式（3）中，约束条件是缓存的平均占用率，因此缓存中的条目有可能超过 B。在第 IV-C
节中，我们讨论了如何分配缓冲空间以防止违反缓存大小
。大小为 ϵB 的缓冲区被添加到大小为 B 的缓存中，只有当缓存中的条目数超过 (1 + ϵ)B 时，才会强制驱逐内容。
在此，我们模拟了以 TTL 方式实现的 LRU 缓存缓冲。回顾第 IV-C 节，当 B(N) = o(N) 和 ϵ2 B(N) =
ω(1) 时，违反缓冲区约束的概率消失。请注意，这里
smax = 1。要为一组 N 个内容提供服务，我们将平均缓存大小设为 B = N、
命中概率 hi
= 1/α ，与物体大小无关 si 。
并分配额外的缓冲区，方法是设置 ϵ =
1.图 2(a)此外，α 将收敛到 ΣjΣsj /B，以维持缓存。
显示了由于以下原因而导致内容被强制驱逐的概率需要注意的是，通过上述对 δm (ti , α) 和 δh (ti , α) 函数的选择，无需了解请求到达率 λi ，即可实现最大最小公平性。
D. 估计 λi
在本节讨论的算法中，计算定时器参数 ti 需要知道大多数策略的请求到达率。如果（高速缓存）服务提供商不知道请求到达率，可以使用估算技术来估算请求到达率。
让 ri 表示文件 i 的剩余 TTL 时间。请注意，ri 可根据 ti
和上次请求文件 i 的时间戳计算得出。让 τi 表示与文件 i 的请求到达时间相关的随机变量，τ¯i 是其平均值。我们可以将平均到达时间近似为 τˆ¯i = ti - ri 。请注意，以这种方式定义的 τ¯ˆi 为高速缓存大小违规，随着系统参数的增加而减少
x

了。图 2(b) 显示，随着缓存大小和文件数量的增加，为防止强制驱逐而分配的额外缓存空间的百分比也在减少
。请注意，这些结果与第 IV-C 节中的定理 1 相一致。
A. 弹性缓存大小在第 IV-B 节中，我们提出了对缓存大小没有严格限制的效用驱动缓存问题。在这种情况下，缓存大小可以任意增大，并产生存储成本。最佳缓存大小由 (10) 计算得出，以获得最大效用减去成本。
在此，我们使用第六节中为 LRU 计算的效用函数来确定最佳缓存大小。图 3 显示了最佳缓存大小与执行 LRU
策略的缓存上总负载的函数关系，其中惩罚函数可以是线性、二次方、三次方或指数函数：0 01
是 τi 的单样本无偏估计器。因此， τi 是一个 "无偏估计器",⎨ 0.01 x2
在计算定时器参数以评估我们的算法时，我们会使用这个估计器。

1/λi 的无偏估计值。在模拟部分，我们将
C(x) =λi对于最大最小公平策略，我们将计时器设置为
t = -1 log (1 - 1)。λi图.
为 LRU 缓存调整缓存大小，使效用-成本权衡最大化。对四种不同的成本函数进行了评估。
图 4：
最佳缓存大小与 (a) 请求到达率和 (b) 请求到达率的函数关系。
(b) 来自 β 公平效用函数的公平参数 β。
最初的缓存大小设置为 B = 100，请求是针对 N =
1000 个文件生成的，这些文件的流行度遵循参数为 0.8
的 Zipf 分布。
我们继续使用线性成本函数，并比较四种缓存策略下的最佳缓存大小：比例公平、LRU、LFU 和 FIFO。文件数假设为 N = 104。图 4(a) 显示了上述政策下最佳缓存大小随到达请求率的变化情况。我们可以看到，最佳缓存大小是请求率的递增函数。这是意料之中的，因为随着请求率的增加，缓存会产生更大的效用。
为了了解缓存策略的选择对最佳缓存大小的影响，我们采用了 β 公平效用函数，并计算了最佳缓存大小与 β 的函数关系。 图 4(b) 显示了四种请求到达率下的结果。我们发现，随着 β 的增加，最佳缓存大小也在增加。
B. 在线算法根据我们在第六节中的讨论，非复位和复位 TTL 高速缓存可以使用 ti = T, i = 1,...N 来实现与 FIFO 和 LRU 高速缓存具有相同统计特性的高速缓存。然而，以前的方法需要预先计算缓存特性时间 T。通过使用第 VII-A 节中开发的在线对偶算法，我们可以在没有 T 的先验信息的情况下实现这些策略。为此，我们实施了非复位和复位 TTL 缓存，将所有文件的定时器参数设置为 ti = 1/α，其中 α 表示对偶变量，并根据 (15) 进行更新。
对于比例公平策略，计时器参数设置为至
t = -1 log (1 - λi)、∝×
我们以重置 TTL 缓存的方式实施比例公平和最大最小公平政策。
在接下来的实验中，我们考虑将缓存中文件的预期数量设置为 B = 1000。对 N = 104 个文件的请求根据泊松过程到达，总速率为 i λi = 1。文件流行度遵循参数
z = 0.8 的 Zipf 分布，即 λi 1/i 。z
在计算定时器参数时，我们使用文件请求率的 估计值，详见第 VII-D 节。
图 5 比较了我们的在线对偶算法与数值计算得出的上述四种策略的命中概率。很明显，对于 FIFO、LRU
和最大最小公平策略，在线算法能获得精确的命中概率
。但是，对于比例公平策略，模拟命中概率与数值计算值并不完全一致。这是由于在估计 λi ,i = 1,...,N 时出现了误差。请注意，我们在这里使用的是一个简单的估计器，它产生的估计值对 1/λi 是无偏的。但是，估计值的倒数并不是对 λi 的无偏估计值。从上式可以看出，计算最大-最小公平政策的定时器参数只需要估计 1/λi，因此结果是好的。另一方面，比例公平策略也需要估计 λi ，因此使用有偏差的 λi 估计会带来一些误差。
为了证实上述推论，我们还模拟了假设对请求率的了解是完美的，则模拟结果与数值完全吻合。图 6 显示，在这种情况下，模拟结果与数值完全吻合。
我们还可以使用基元二元算法来实施比例公平策略。
在此，我们使用 (18) 中的更新规则和请求率的估计值来实施该策略。图 7 显示，与二元算法不同，模拟结果与数值相符。这个例子说明，在实施特定策略时，一种算法可能比其他算法更理想。
第七节中解释的算法被证明是全局稳定和渐进稳定的，并能收敛到最优解。图 8(a) 显示了 LRU 策略的对偶变量收敛情况 。图中红线表示 1/T = 6.8 10-4，其中 T 是根据第六节的讨论计算出的 LRU 缓存的特征时间。图
8(b) 还显示了高速缓存中内容的数量是如何以容量为中心的。
B.缓存中文件数量的概率密度和互补累积分布函数（
CCDF）如图 9 所示。违反容量 B 超过 10% 的概率小于
2.5 10-4。对于较大的系统，即 B 和 N 较大时，违反目标缓存容量的概率会变得非常小；参见第 IV-C 节的讨论。
这也是我们在模拟中观察到的结果。在执行其他策略时，我们也观察到了类似的双变量收敛和缓存大小收敛行为。KLh图 5.使用估计的 λi 对 LRU、FIFO、比例公平和最大最小公平策略使用效用函数执行在线对偶算法的命中概率。红线对应数值评估值，蓝色曲线显示模拟获得的值。
图 6.使用对偶算法实施的比例公平策略，精确了解 λ s。i
图 7.使用δm (ti , α) = λi 和 δh (ti , α) = α λi 的原始二元算法实施的比例公平策略，其中 λi 为近似值。
图 8.代表 LRU 策略的效用函数的对偶算法的收敛性和稳定性。
为了了解这些算法在得出最优解方面的对比情况，我们计算了计算出的最优解和计算出的最优解之间的命中概率差异。
hi ，以及算法在不同迭代阶段的命中概率 hˆ i。计算两个概率分布之间的距离、
我们使用库尔巴克-莱伯勒（KL）发散度量，定义如下= Σi=1hˆ i，在算法的不同迭代阶段。
图 9.采用代表 LRU 策略的效用函数的对偶算法的缓存大小分布和
图 10.假设精确了解请求率，在算法迭代过程中，(a) 比例公平策略和
(b) 最大最小公平策略由对偶算法和基元对偶算法执行时与最优策略的偏差。
图 10 显示了双算法和初等双算法假定准确了解请求率， 在 实施比例公平和最大最小公平策略时，随着时间的推移与最优策略的偏离情况。据观察，这两种算法在两种策略下的表现类似。
然后，我们比较了在请求到达率未知但有估计值的情况下的算法。图 11 显示了随时间变化的分歧。在这种情况下，在执行比例公平策略时，原始-二元算法的性能远远优于二元算法。对于最大最小公平策略，这两种算法的表现类似。
C. 非复位与复位 TTL
第七节中描述的在线算法可以作为非复位或复位 TTL
高速缓存实现。在此，我们通过实验来了解 TTL 类型的选择如何影响这些算法的性能。我们采用比例公平策略和最大最小公平策略，并使用基元二元算法将它们分别作为重置和非重置 TTL 缓存来实现。利用 KL-发散度量，我们计算了离线计算出的最优解 hi 与通过以下方法计算出的命中概率之间的差异。
图 11.在算法迭代中，(a) 亲公平策略和 (b) 最大-最小公平策略在估计请求率的情况 下，由对偶算法和原始-对偶算法执行时与最优策略的偏差图 13.基于替换算法实现的 LRU 和基于对偶算法实现的 LRU 在命中次数上的相对误差。
不过，也可以将效用定义为字节命中概率的函数，即 Ui (s
hii)。在这种情况下，可以使用以下公式来实施效用驱动型缓存策略：最大化
12.在算法迭代中，(a) 亲公平（pro-portionally fair）和(b) 最大最小公平（max-min fair）策略在 priml-dual 算法中作为非复位和复位 TTL 缓存执行时与最优策略的偏差。
图 12 显示了非复位和复位 TTL 实现的分歧如何随时间推移而减小。我们注意到，这两种实现方式向最优解收敛的速度几乎相同。
D. 轨迹驱动模拟在本节的前半部分，我们展示了 LRU 策略可以使用双重算法作为 TTL 缓存来实现。为了证明双重算法可以在现实环境中使用，我们使用了从 IBM 研究实验室的网关路由器[39]中收集到的网络访问跟踪请求。我们使用跟踪来计算基于替换的 LRU 实现（hR ）和基于双重算法的实现（hD ）的高速缓存命中率。我们在 W = 3000 个请求的窗口中计算每种实现的命中率，并计算相对误差为相对误差 = hc - hD 。图 13 显示了随时间变化的相对误差。很明显，相对误差很小，这意味着基于对偶算法的 LRU 实现效果接近于基于替换算法的实现效果。
i=1这样 i=1 0 ≤ hi ≤ 1，i = 1，2，...，N、
上述表述的含义及其与(3)的区别需要进一步研究。
B.分解第 IV 部分的问题表述假设系统已知 Ui () 的效用函数。
在现实中，内容提供商可能决定不与服务提供商共享其效用函数。为了处理这种情况，我们将优化问题 (3) 分解成两个更简单的问题。
假设缓存存储作为一项服务提供，服务提供商按固定费率 r 向内容提供商收取存储空间费。因此，内容提供商需要支付 wi = rhi 才能获得文件 i 的命中概率 hi 。
文件 i 的内容提供商的效用最大化问题可写成最大化 Ui (wi /r) - wi
使得 wi ≥ 0
(19) 现在，假设服务提供商知道向量 w，为了按比例公平分配资源，应根据以下公式设置命中概率IX. 讨论在 本节中，我们将探讨效用驱动型缓存对缓存服务货币化的影响，并讨论一些未来的研究方向。
(20) i=1
A.文件大小不一致第 IV 节中的效用最大化框架假定效用被定义为文件命中概率 hi 的函数。
文献[14]表明，总是存在这样的向量 w 和 h：w 解 (19)，h 解 (20)；此外，向量 h 是 (3) 的唯一解。
C.成本和效用函数在第 IV-B 节中，我们定义了一个惩罚函数， 表示使用额外存储空间的成本。我们还可以根据检索未缓存内容所消耗的网络带宽来定义成本函数。这在模拟网络缓存时尤其有趣，因为网络链接很可能会出现拥塞。
优化问题 (3) 使用定义为命中概率函数的效用函数。将效用定义为命中率的函数是合理的。这将如何影响问题，例如公平性的概念，是一个需要进一步研究的问题。支持将效用定义为命中率函数的一个论点是，服务提供商可能更倾向于根据请求率而不是缓存占用率来定价。此外，在设计分层缓存时，服务提供商的目标可能是最大限度地降低内部带宽成本。这可以通过将效用函数定义为 Ui = Ci
(mi) 来实现，其中 Ci (mi) 表示与文件 i 的未命中率 mi
相关的成本。
D.在线算法在第七部分，我们开发了三种在线算法，可用于实施效用驱动型缓存。虽然这些算法都被证明是稳定的，并能收敛到最优解，但它们都有各自的特点，能让一种算法更有效地实施策略。例如，基于二元解法实施最大最小公平策略需要知道/估计文件请求率，而使用修改后的原始二元解法则不需要知道/估计文件请求率。此外，对于不同的策略，这些算法的收敛速度也可能不同。选择非复位或复位 TTL 缓存也会对这些算法的设计和性能产生影响。这些都是需要进一步研究的课题。
E.非复位与复位 TTL
如第三节所述，效用最大化缓存策略可以通过重置或非重置 TTL 缓存来实现。TTL 类型的选择可能会影响实施的难易程度、对到达率估计的敏感度或算法的收敛时间。
例如，TTL 复位是实现 LRU 的更好选择，而非复位 TTL
则是实现 FIFO 的更好选择。第八节中的图 12 显示，采用重置或非重置 TTL 缓存，实施比例公平策略和最大最小公平策略可以获得相同的精度。正确选择 TTL 类型可以简化算法的实施，LRU 和 FIFO 策略也是如此。
X. 结论在本文中，我们提出了效用驱动缓存的概念，并将其表述为一个优化问题，其中包括
i图 14. 使用非重置和重置 TTL 高速缓存对给定文件请求的高速缓存命中和未命中情况。(a) 定时器 ti 的非重置 TTL 缓存。 (b) 定时器 ti 的重置 TTL 缓存。
我们开发了三种去中心化算法，以在线方式实施效用驱动型缓存策略，并能适应文件请求率随时间的变化。
我们证明了这些算法是全局稳定的，并能收敛到最优解
。通过模拟，我们展示了这些算法的效率和我们方法的灵活性。
附录 A
TTL 缓存命中概率图 14 显示了在(a) 非重置和(b) 重置 TTL 缓存情况下请求文件的缓存动态，假设文件 i 的定时器等于 ti 。
我们只模拟单个文件的动态，因为我们考虑的是没有容量限制的 TTL 缓存。每种策略的 ti 值将根据缓存容量确定。
观察图 14 ，我们可以发现，文件的缓存占用过程可分为多个周期，这些周期之间以缓存未命中为分界。请注意，这些周期在静态上是相同的。
参考资料
[1] 思科视觉网络指数：2014-2019 年预测与方法》，思科，美国加利福尼亚州圣何塞，2015 年 5 月。
[2] J.G. Andrews、H. Claussen、M. Dohler、S. Rangan 和 M. C. Reed，" Femtocells：过去、现在和未来》，IEEE J. Sel.Areas Commun.》，第 30 卷，第 3 期，第 497-508 页。3, pp.
[3] N.Golrezaei, K. Shanmugam, A. G. Dimakis, A. F. Molisch, and G.
Caire, "FemtoCaching: Wireless video content delivery through
distributed caching helpers," in Proc. INFOCOM, Mar. 2012、pp.1107-1115.
[4] S.Borst, V. Gupta, and A. Walid, "Distributed caching algorithms for
content distribution networks," in Proc. INFOCOM, Mar. 2010, pp.
[5] Y.Lu, T. F. Abdelzaher, and A. Saxena, "Design, implementation, and
evaluation of differentiated caching services," IEEE Trans.ParallelDistrib.Syst.5, pp.
[6] M.Feldman and J. Chuang, "Service differentiation in Web caching and
content distribution," in Proc.CCN, 2002, pp.
[7] B.-J. Ko 、 K.-W.Lee, K. Amiri, and S. Calo, "Scalable service dif-
ferentiation in a shared storage cache," in Proc. ICDCS, May 2003、pp.184-193.
[8] T.Kelly, Y. M. Chan, S. Jamin, and J. K. MacKie-Mason, "Biased
replacement policies for Web caches ： Differential quality-of-service
and aggregate user value," in Proc.网络缓存研讨会，1999 年，第 1-16 页。
[9] N.C. Fofack, P. Nain, G. Neglia, and D. Towsley, "Performance
e v a l u a t i o n of hierarchical TTL-based cache networks," Comput.
网络》，第 65 卷，第 212-231 页，2014 年 6 月。
[10] R.Fagin, "Asymptotic miss ratios over independent references," J. Com-
put.Syst.14, no. 2, pp.
[11] H.Che, Z. Wang, and Y. Tung, "Analysis and Design of hierarchical
Web caching systems," in Proc. INFOCOM, Apr. 2001, pp.
[12] R.Srikant and L. Ying, Communication Networks：An Optimiza- tion,
Control, and Stochastic Networks Perspective.英国剑桥：剑桥大学出版社，2013 年。
[13] M.J. Neely, Stochastic Network Optimization With Application to
Communication and Queueing Systems.美国加利福尼亚州圣拉斐尔：
摩根克莱普尔出版社，2010 年。
[14] F.Kelly,"Chargin gandrate control for elastictraffic,"
Eur.Telecommun.8, no. 1, pp.1997.
[15] F.F. P. Kelly, A. K. Maulloo, and D. K. H. Tan, "Rate control for
communication networks ： Shadow prices, proportional fairness and
stability," J. Oper.Soc., vol. 49, no.3, pp.
[16] L.Tassiulas and A. Ephremides, "Stability properties of constrained
queueing systems and scheduling policies for maximum throughput in
multihop radio networks," IEEE Trans.Autom.控制》，第 37 卷，第
12 期、
pp.1936-1948 年，1992 年 12 月。
[17] A.Eryilmaz and R. Srikant, "Fair resource allocation in wireless
networks using queue-length-based scheduling and congestion control,"
IEEE/ACM Trans.网络》，第 15 卷，第 6 期，第 1333-1344 页，2007 年 12 月。
[18] M.J. Neely, E. Modiano, and C. E. Rohrs, "Dynamic power allocation
and routing for time varying wireless networks," in Proc. INFOCOM,Mar./Apr. 2003, pp.
[19] L.Huang and M. J. Neely, "Utility optimal scheduling in energy-
harvesting networks," IEEE/ACM Trans.Netw.4,
pp.1117-1130, Aug. 2013.
[20] M.K. Qureshi 和 Y. N. Patt，"基于实用程序的高速缓存分区：一种低开销、高性能的运行时共享缓存分区机制"，Proc. IEEE/ACM
39th Annu.Int.Symp.2006 年 12 月，第 423-432 页。
[21] G. E. Suh, L. Rudolph, and S. Devadas, "Dynamic partitioning of shared
cache memory," J. Supercomput.
[22] S.Kim, D. Chandra, and Y. Solihin, "Fair cache sharing and partitioning
in a chip multiprocessor architecture," in Proc、pp.111-122.
[23] R.T. Ma 和 D. Towsley, "Cashing in on caching: On-demand contract
design with linear pricing," in Proc.Emerg.Netw.Exp. Technol.，2015 年
12 月，第 8 页。
[24] M. Dehghan, L . Massoulie, D . Towsley, D . Menasche, and
Y.Y. C. Tay ， " 网络 缓存 设计 的效 用优 化方法 " ， 载于 Proc.
INFOCOM，2016 年 4 月，第 1-9 页。
[25] G. Neglia et al., "Access-time-aware cache algorithms," ACM
Trans.Model.Perform.Eval.Comput.Syst.》，第 2 卷，第 21 页，2017
年 12 月。
[26] G. Neglia, D. Carra, and P. Michiardi, "Cache policies for linear utility
maximization," IEEE/ACM Trans.Netw.》，第 26 卷，第 1 期，第 302-
313 页，2018 年 2 月。
[27] J.Jung, A. W. Berger, and H. Balakrishnan, "Modeling TTL-based
Internet caches," in Proc. INFOCOM, Mar. 2003, pp.
[28] C.Fricker, P. Robert, and J. Roberts, "A versatile and accurate
approxi- mation for LRU cache performance," in Proc. ITC, Sep.2012, pp.
[29] G. Bianchi、A. Detti、A. Caponi 和 N. Blefari-Melazzi，"存储前检查：LRU 缓存中内容完整性验证的性能代价是什么？ACM
SIGCOMM Comput.Rev., vol. 43, no.Rev., vol. 43, no.3, pp.
[30] N.N. C. Fofack, M. Dehghan, D. Towsley, M. Badov, and D. L.
Goeckel, "On the performance of general cache networks," in
Proc.Conf.Perform.Eval.Methodol.工具》，2014 年 12 月，第 106-
113 页。
[31] D.S. Berger, P. Gland, S. Singla, and F. Ciucu, "Exact analysis of TTL
cache networks," Perform.Eval.》，第 79 卷，第 2-23 页，2014 年
9 月。
[32] A.Ferragut, I. Rodríguez, and F. Paganini, "Optimizing TTL caches
under heavy-tailed demands," in Proc.ACM SIGMETRICS，2016 年pp.101-112.
[33] A.Ferragut, I. Rodríguez, and F. Paganini, "Optimal timer-based
caching policies for general arrival processes," Queueing Syst、pp.207-241, Apr.
[34] L.Wang, G. Tyson, J. Kangasharju, and J. Crowcroft, "FairCache：在
ICN 缓存中引入公平性"，Proc. IEEE 24th Int.Conf.Netw.Protocols
(ICNP)，2016 年 11 月，第 1-10 页。
[35] W.Chu, M. Dehghan, D. Towsley, and Z.-L. Zhang, "On allocating
cache resources to content providers," in Proc.ACM Conf.2016 年，第 154-159 页。
[36] N.K. Panigrahy、J. Li 和 D. Towsley，"基于命中率与命中概率的高速缓存效用最大化"，SIGMETRICS Perform.Eval.Rev.》，第 45
卷，第 2 期，第 21-23 页，2017 年 10 月。
[37] N.N. C. Fofack、D. Towsley、M. Badov、M. Dehghan 和 D. L.
Goeckel，"异构和通用高速缓存网络的近似分析"，法国巴黎英瑞亚公司，技术报告 hal-00975339，2014 年 4 月。Hal-00975339，2014 年 4 月。
[38] J.Mo and J. Walrand, "Fair end-to-end window-based congestion
control," IEEE/ACM Trans.Netw.5, pp.
[39] P.Zerfos, M. Srivatsa, H. Yu, D. Dennerline, H. Franke, and
D.Agrawal, "Platform and Applications for massive-scale stream- ing
network analytics," IBM J. Res.Develop.3/4,
pp.11:1-11:13, May/Jul. 2013.
[40] D.Feijer 和 F. Paganini，"原始双梯度动力学的稳定性及在网络优化中的应用"，《自动》，第 46 卷，第 12 期、pp.1974-1981, 2010.
Mostafa Dehghan 在 卡尔加里大学获得硕士学位，期间他开发了 合作网络中的分布式路由算法；在马萨诸塞大学阿默斯特分校信息与计算机科学学院获得博士学位，期间他参与了以内容为中心的网络的建模和性能评估、缓存管理
、缓存网络中的路由选择以及 CCN 数据 结构 分析。他目前在谷歌公司工作。
Laurent Massoulié 1991 年毕业于法国帕拉伊索理工学院，1995 年获得法国奥尔赛南巴黎大学自动控制博士学位。
他目前是法国国家科学研究院的研究员，领导着微软研究院-法国国家科学研究院联合中心，同时也是法国综合理工学院应用数学中心的教授他曾于 1995 年至 1999 年在法国电信研发部任职，1999 年至 2006 年在英国剑桥微软研究院任职，以及在巴黎 Technicolor 公司任职、
2006年至2012年，他曾担任《队列系统：排队系统》一书的副主编。
2000 年至 2006 年，他担任《排队系统》（Queueing Systems：理论与应用 》 杂志 的副 主编 ， 2008 年担任 《 IEEE/ACM TRANSACTIONS ON
NETWORKING》杂志的副主编，2011 年起担任《Stochastic Systems》杂志的副主编 。 他与 他人 合作 撰写了 IEEE INFOCOM 1999 、 ACM
SIGMETRICS 2005 和 ACM CoNEXT 2007 的最佳论文奖获奖论文、
他曾获得 2018 年 NeurIPS 奖，2011 年当选 Technicolor Fellow，2017 年获得 Del Duca 基金会科学大奖（Grand Prix Scientifique）。他的研究重点是机器学习和大型网络（包括 P2P 和社交网络）的概率建模和算法设计。
Don Towsley（LF'15）分别于 1971 年和 1975 年获得德克萨斯大学物理学学士学位和计算机科学博士学位。1976 年至 1985 年，他在美国马萨诸塞州阿默斯 特市的马萨诸塞大学阿默斯特分校电气与计算机工程系任教 ， 现为计算机科学系特聘教授。他曾在美国 纽约州约克敦高地的
IBM T. J. 沃森研究中心、巴黎的 MASI 实验室担任访问学者、
他的研究兴趣包括网络和性能评估。他已当选为 ACM 会员。他还是
ORSA 的成员。他曾获得 1998 年 IEEE 通信学会 William Bennett 最佳论文奖、2007 年 IEEE Koji Kobayashi 奖、2007 年 ACM SIGMETRICS 成就奖以及众多最佳会议/研讨会论文奖。他是 ACM SIGMETRICS 和
PERFORMANCE' 92 联合会议以及 Performance 2002 会议的联合主席。
他是 IFIP 7.3 工作 组主席 。 他曾 担任 IEEE/ACM TRANSACTIONS ON
NETWORKING 的主编 ， 还是 ACM 期刊和 IEEE JOURNAL ON SELECTED
AREAS IN COMMUNICATIONS 的编辑委员会成员。此前，他还担任过许多其他编辑委员会的成员。
Y.Y. C. Tay 获得新加坡国立大学理学士学位和哈佛大学博士学位。他目前是新加坡国立大学数学系教授，也是新加坡国立大学计算机科学系教授，同时还是腾布苏学院的驻校研究员。他曾在普林斯顿大学、麻省理工学院、剑桥大学、加州大学洛杉矶分校、国立台湾大学、微软、英特尔和 VMware 休学。他著有《计算机系统性能分析建模》（摩根大通出版社，2007 年）。
& Claypool，第 3 版）。他的主要研究兴趣是性能建模（数据库事务、
无线协议、互联网流量和缓存缺失）。其他兴趣还包括数据库系统（数据和社交网络的合成生成）以及分布式计算中本地时间的使用。他曾担任 ACM SIGMETRICS、ACM SIGMOD、IFIP PERFORMANCE、VLDB
、IEEE ICDE、IEEE MASCOTS、IFIP NETWORK- 和 IFIP NETWORK-
的项目委员会成员。
ING 和 ICS。他还是 ACM Transactions 的高级副主编。
计算系统建模与性能评估》。
丹尼尔-萨多克-梅纳施（Daniel Sadoc Menasché
）于 2011 年获得马萨诸塞大学阿默斯特分校计算机科学博士学位。他目前是 巴西里约热内卢联邦大学计算机科学系助理教授。他的兴趣是计算机系统的建模、分析和性能评估。他曾获得
2007 年 GLOBECOM、2009 年 CoNEXT、2013
年 INFOCOM 和 2015 年 ICGSE 的最佳论文奖。
他是目前是巴西科学院。
