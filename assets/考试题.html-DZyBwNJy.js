import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,d as p,o as n}from"./app-C9ZstRnq.js";const i={};function d(r,e){return n(),s("div",null,[...e[0]||(e[0]=[p(`<h1 id="考试2021" tabindex="-1"><a class="header-anchor" href="#考试2021"><span>考试2021</span></a></h1><h2 id="_4-1试述大数据的四种计算模式-并列举一种代表产品" tabindex="-1"><a class="header-anchor" href="#_4-1试述大数据的四种计算模式-并列举一种代表产品"><span>4.1试述大数据的四种计算模式，并列举一种代表产品</span></a></h2><h3 id="批处理计算" tabindex="-1"><a class="header-anchor" href="#批处理计算"><span>批处理计算</span></a></h3><p>解释：针对大规模数据的批量处理</p><p>代表产品：MapReduce Spark</p><h3 id="流计算" tabindex="-1"><a class="header-anchor" href="#流计算"><span>流计算</span></a></h3><p>针对流数据的实时计算</p><p>Storm</p><h3 id="图计算" tabindex="-1"><a class="header-anchor" href="#图计算"><span>图计算</span></a></h3><p>针对大规模图数据结构的处理</p><p>Pregel GraphX</p><h3 id="查询分析计算" tabindex="-1"><a class="header-anchor" href="#查询分析计算"><span>查询分析计算</span></a></h3><p>大规模数据的存储管理，查询分析</p><p>Hive Dremel</p><h2 id="_4-2-hadoop2-0的改进" tabindex="-1"><a class="header-anchor" href="#_4-2-hadoop2-0的改进"><span>4.2 Hadoop2.0的改进</span></a></h2><p>在 Hadoop1.x 中的 NameNode 只可能有一个，虽然可以通过SecondaryNameNode 与 NameNode进行数据同步备份，</p><p>但是总会存在一定的时延，如果 NameNode 挂掉，但是如果有部分数据还没有同步到SecondaryNameNode 上，还是可能会存在着数据丢失的问题。</p><p>在Hadoop2.X 中， HDFS 的变化，主要体现在增强了 NameNode 的水平扩及可用性，可以同时部署多个NameNode ，这些NameNodes 之间是相互独立，也就是说他们不需要相互协调， DataNode 同时在所有NameNodes 注册，做为他们共有的存储节占并向定时向所有的这些NameNodes 发送心跳块使用情况的报告，并处理所有 NameNodes 向其发送的指令</p><h2 id="_4-3-描述spark生态系统包含的组件及应用场景" tabindex="-1"><a class="header-anchor" href="#_4-3-描述spark生态系统包含的组件及应用场景"><span>4.3 描述Spark生态系统包含的组件及应用场景</span></a></h2><p>Spark Core 是整个 BDAS 生态系统的核心组件，是一个分布式大数据处理框架。Spark Streaming 是一个对实时数据流进行高吞吐、高容错的流式处理系统，可以对多种数据源（如 Kafka 、Flume 、 Twitter 和 ZeroMQ 等）进行类似 Map 、 Reduce 和 Join 等复杂操作，并将结果保存到外部文件系统、数据库或应用到实时仪表盘。Shark 即 Hive on Spark, 本质上是通过 Hive 的 HQL 进行解析，把 HQL 翻译成 Spark 上对应的 RDD 操作，然后通过 Hive 的 Metadata 获取数据库里的表信息，实际为 HDFS 上的数据和文件，最后由 Shark 获取并放到Spark 上运算。BlinkDB 是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。MLBase 是 Spark 生态系统中专注于机器学习的组件，它的目标是让机器学习的门槛更低，让一些可能并不了解机器学习的用户能够方便地使用MLBase。</p><p>MLBase 分为四个部分MLRuntime 、MLlib 、 ML和 MLOptimizer。</p><h2 id="_4-4-nosql数据库的四大类型" tabindex="-1"><a class="header-anchor" href="#_4-4-nosql数据库的四大类型"><span>4.4 NoSql数据库的四大类型</span></a></h2><p>一、键值 (Key-Value) 数据库</p><p>键值数据库就像在传统语言中使用的哈希表。你可以通过key来添加、查询或者删除数据，鉴于使用主键访问，所以会获得不错的性能及扩展性。</p><p>二、面向文档 (Document-Oriented) 数据库</p><p>面向文档数据库会将数据以文档的形式储存。每个文档都是自包含的数据单元，是一系列数据项的集合。每个数据项都有一个名称与对应的值，值既可以是简单的数据类型，如宇符串、数字和日期等;也可以是复杂的类型，如有序列表和关联对象。数据存储的最小位是文档，同一个表中存储的文档属性可以是不同的，数据可以使用XML、JSON或者JSONB等多种形式存储。</p><p>三、列存储 (Wide Column Store/Column-Family) 数据库</p><p>列存储数据库将数据储存在列族 (column family) 中，一个列族存储经常被一起查询的相关数据。举个例子，如果我们有一个Person类，我们通常会一起查询他们的姓名和年齿而不是薪资。这种情况下，姓名和年龄就会被放入一个列族中，而薪资则在另一个列族</p><p>四、图 (Graph-Oriented) 数据库</p><p>图数据库允许我们将数据以图的方式储存。实体会被作为顶点，而实体之间的关系则会被作为边。比如我们有三个实体，Steve Jobs、Apple和Next，则会有两个“Foundedby”的边将Apple和Next连接到Steve Jobs。</p><h2 id="_4-5-简述hdfs的第二名称节点辅助名称节点进行fsimage和editlog合并的过程。" tabindex="-1"><a class="header-anchor" href="#_4-5-简述hdfs的第二名称节点辅助名称节点进行fsimage和editlog合并的过程。"><span>4.5 简述HDFS的第二名称节点辅助名称节点进行FsImage和EditLog合并的过程。</span></a></h2><p>四5 名称节点运行期间，HDFS内的更新操作被写入到EditLog文件中，随着更新操作的不断发生，EditLog也将不断变大。名称节点在每次重启时，将Fslmage加载到内存中，再逐条执行EditLog中的记录，使Fslmage保持最新状态。每隔一段时间. SecondarvNameNode会与NameNode进行通信，请求 NameNode停止使用EditLog文件，让NameNode将这之后新到达的写操作写入到一个新的文件EditLog.new中;然后 SecondaryNameNode将EditLog、Fslmage拉回至本地，加载到内存中-即将Fslmage加载到内存中，再逐条执行EditLog中的记录，使Fslmage保持最新。</p><h2 id="_4-6对比mapreduce-spark-streaming-storm实时响应的能力" tabindex="-1"><a class="header-anchor" href="#_4-6对比mapreduce-spark-streaming-storm实时响应的能力"><span>4.6对比MapReduce Spark Streaming Storm实时响应的能力</span></a></h2><p>四6Hadoop MapReduce 是三者中出现最早，知名度最大的分布式计算框架。主要适用于大批量的集群任务由于是批量执行，故时效性偏低</p><p>Spark Streaming 保留了 HadoopMapReduce 的优点，而且在时效性上有了很大提高，中间结果可以保存在内存中，从而对需要迭代计算和有较高时效性要求的系统提供了很好的支持，多用于能容忍小延时的推荐与计算系统。</p><p>Storm一开始就是为实时处理设计，因此在实时分析/性能监测等需要高时效性的领域广泛采用。</p><h2 id="_5" tabindex="-1"><a class="header-anchor" href="#_5"><span>5</span></a></h2><p>配置SSH无密码 <code>ssh-keygen -t rsa</code></p><p>环境变量生效<code> source /etc/profile</code></p><p>slave文件的文本：<code>四行</code></p><p>分别是</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>4.</span></span>
<span class="line"><span>- core.site.xml</span></span>
<span class="line"><span>- hdfs.site.xml</span></span>
<span class="line"><span>- mapreduce.site.xml</span></span>
<span class="line"><span>- yarm.site.xml</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>// 5.</span></span>
<span class="line"><span>&lt;configuration&gt;  </span></span>
<span class="line"><span>  &lt;property&gt;  </span></span>
<span class="line"><span>    &lt;name&gt;fs.defaultFS&lt;/name&gt;  </span></span>
<span class="line"><span>    &lt;value&gt;hdfs://192.168.1.100:2200&lt;/value&gt;  </span></span>
<span class="line"><span>  &lt;/property&gt;  </span></span>
<span class="line"><span>&lt;/configuration&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>//6. yarn-site.xml</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="8"><li></li></ol><p>名称节点格式化<code>hdfs namenode -format</code></p><p>启动hdfs<code>start-dfs.sh</code></p><p>启动yarn<code>start-yarn.sh</code></p><ol start="9"><li></li></ol><p>1200</p><p>六</p><ol><li></li></ol><p>完成hdfs shell命令</p><p>hadoop fs -mkdir /user/hadoop/dir1</p><p>hadoop fs -put File.txt /user/hadoop/dir1</p><p>hdfs dfs -cat hdfs://user/hadoop/dir1/File.txt</p><ol start="2"><li></li></ol><p><code>put &#39;student&#39;, &#39;9528&#39;, &#39;course:math&#39;, &#39;88&#39;</code></p><p><code>get &#39;student&#39;, &#39;9528&#39;, &#39;course:math&#39;</code></p><p>3.(2_1)</p><p>基于Hadoop MapReduce的程序</p><p>6-10</p><p>6 <code>public static class ScoreMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;</code></p><p>7 <code>stu_name, score</code></p><p>8 <code>sum += val.get(); </code></p><p>9 <code>ScoreSumMapper.class</code></p><p>10 <code>ScoreSumReducer.class</code></p>`,67)])])}const t=a(i,[["render",d]]),c=JSON.parse('{"path":"/%E5%BD%92%E6%A1%A3/%E5%AD%A6%E6%A0%A1/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E8%80%83%E8%AF%95%E9%A2%98.html","title":"考试2021","lang":"zh-CN","frontmatter":{"feed":false,"seo":false},"git":{"createdTime":1758333160000,"updatedTime":1758333160000,"contributors":[{"name":"sxh","username":"sxh","email":"2362989228@qq.com","commits":1,"url":"https://github.com/sxh"}]},"readingTime":{"minutes":5.25,"words":1576},"filePathRelative":"归档/学校/大数据技术/考试题.md"}');export{t as comp,c as data};
